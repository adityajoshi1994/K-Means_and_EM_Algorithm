import pandas as pd
import numpy as np
from scipy import linalg
import random as r
import matplotlib.pyplot as plt
import matplotlib.pyplot as plt1
import matplotlib as mpl
import math
import K_Means
from matplotlib import cm
from matplotlib.patches import Ellipse
import matplotlib.pyplot as plt
ax = plt.subplot(111, aspect='equal')

def plotCluster(list,mu,sigma):
    plt.cla()
    colors = iter(cm.rainbow(np.linspace(0, 1, len(list) * 2)))
    i = 0
    for l in list:
        x = []
        y = []
        for p in l:
            x.append(p[0])
            y.append(p[1])
        c = next(colors)
        plt.scatter(x, y, color=c)
        plt.scatter(mu[i][0],mu[i][1],color = next(colors))
        lambda_, v = np.linalg.eig(sigma[i])
        lambda_ = np.sqrt(lambda_)
        theta = np.degrees(np.arctan2(*v[:, 0][::-1]))
        ell = Ellipse(mu[i],
                      width=lambda_[0] * 2 * 2, height=lambda_[1] * 2 * 2,
                      angle=theta)
        ell.set_facecolor('none')
        ell.set_linewidth(2)
        ell.set_edgecolor(c)
        ax.add_artist(ell)

        i += 1

    plt.xlabel('X1')
    plt.ylabel('X2')
    plt.show()
    plt.pause(0.0001)



def plotLikelihood(lls):
    (list,iterator) = lls[0]
    max = list[len(list) - 1]
    for (ll,iter) in lls:
        if ll[len(ll) - 1] > max:
            list = ll
            max = ll[len(ll) - 1]
            iterator = iter
    plt1.plot(iterator, list, '-')
    plt1.xlabel('iterations')
    plt1.ylabel('log-likelihood')
    plt1.show()

def getDistance(d1, d2):
    diff = d1 - d2
    return np.sqrt(diff.T.dot(diff))


def gaussian_mixture(data, K, init_method, epsilon, niterations, plotflag,r, RSEED=123):

    '''

    % Template for a function to fit  a Gaussian mixture model via EM

    % using K mixture components. The data are contained in the N x d "data" matrix.

    %

    % INPUTS

    %  data: N x d real-valued data matrix

    %  K: number of clusters (mixture components)

    %  initialization_method: 1 for memberships, 2 for parameters, 3 for kmeans

    %  epsilon: convergence threshold used to detect convergence

    %  niterations (optional): maximum number of iterations to perform (default 500)

    %  plotflag (optional): equals 1 to plot parameters during learning,

    %                       0 for no plotting (default is 0)

    %  RSEED (optional): initial seed value for the random number generator

    %

    %

    % OUTPUTS

    %  gparams: K-dim structure array containing the learned mixture model parameters:

    %           gparams(k).weight = weight of component k

    %           gparams(k).mean = d-dimensional mean vector for kth component

    %           gparams(k).covariance = d x d covariance vector for kth component

    %  memberships: N x K matrix of probability memberships for "data"

    %

    %  Note: Interpretation of gparams and memberships:

    %

    %    - gparams(k).weight is the probability that a randomly selected row

    %         belongs to component (or cluster) i (so it is "cluster size")

    %

    %    - memberships(i,k) = p(cluster k | x) which is the probability

    %         (computed via Bayes rule) that vector x was generated by cluster

    %         k, according to the "generative" probabilistic model.

    '''



    # your code goes here....
    N = len(data)


    # initialize....
    lls = []
    for i in range(r):

        mu = [np.array([0.,0.]) for j in range(K)]
        sigma = [np.array([[0.,0.],[0.,0.]]) for j in range(K)]
        z = [1/float(K) for i in range(K)]
        eFirst = False
        mFirst = False
        prevLikelihood = 0
        compMat = list()
        if init_method == 0:
            compMat = randomWeights(N,K)
            mFirst = True
        elif init_method == 1:
            mu,sigma = randomParams(len(data[0]),K)
            eFirst = True

        else:
            clusters = K_Means.kMeans(data,K,1)
            mu = [clusters[j].mean for j in range(K)]
            sigma = [np.eye(len(clusters[0].mean)) for i in range(K)]
            eFirst = True
        done = False
        iterations = 0
        plt.ion()
        iter = []
        ll = []
        while not done:

            iterations += 1

        # perform E-step...
            if eFirst:
                list1 = [[] for j in range(K)]
                maxList = []
                compMat = list()
                for d in data:
                    x = [gaussian(d,mu[k],sigma[k]) * z[k] for k in range(K)]
                    x = [j / sum(x) for j in x]
                    compMat.append(x)
                    list1[np.array(x).argmax()].append(d)
                compMat = np.array(compMat)
                plotCluster(list1,mu,sigma)
                mFirst = True

        # perform M-step...
            if mFirst:
                #print len(compMat[:,0])
                z = [sum(compMat[:,k]) / float(len(data)) for k in range(K)]
                mu = [np.array([0., 0.]) for j in range(K)]
                maxDist = 0
                for k in range(K):
                    for j in range(len(data)):

                        mu[k] += compMat[j][k] * data[j]

                    mu[k] /= float((z[k] * len(data)))

                sigma = [np.array([[0.,0.],[0.,0.]]) for j in range(K)]
                for k in range(K):
                    p = z[k]
                    p = p * len(data)
                    for j in range(len(data)):
                        c = (data[j] - mu[k])
                        sigma[k] += compMat[j][k]*(np.mat(c).T * np.mat(c))
                    sigma[k] /= p
                eFirst = True
                if iterations > niterations:
                    done = True


            print "Iteration....", iterations
        # compute log-likelihood and print to screen.....

            final_sum = 0
            for d in data:
                log_likelihood = 0
                for j in range(K):
                    log_likelihood += z[j] * gaussian(d,mu[j],sigma[j])
                final_sum += np.log(log_likelihood)
            ll.append(final_sum)
            if abs(prevLikelihood - final_sum) < epsilon:
                print "Converged after...",iterations
                done = True
            prevLikelihood = final_sum

            iter.append(iterations)
        lls.append((ll,iter))


        # check for convergence.....

        plt.ioff()
        plt.show()
    return lls


def gaussian(d,muk,sigmak):
    diff = (d - muk)
    m = diff.dot(np.linalg.pinv(sigmak)).dot(diff.T)
    #At = np.linalg.pinv(sigmak)
    #m = reduce(np.dot, [d, At, d.T])
    b = np.linalg.det(sigmak)
    if b < 0:
        b *= -1.
    c = 2 * math.pi * b
    s = (1 / c) * math.e ** -m
    return s

def randomWeights(N,k):
    list = []
    for i in range(N):
        x = [r.uniform(0,1) for i in range(k)]
        x = [j / sum(x) for j in x]
        list.append(x)
    #print np.array(list)
    return np.array(list)

def randomParams(s,k):
    mu = [np.random.uniform(low=1., high=10., size=(1,s))[0] for i in range(k)]
    sig = [np.random.uniform(low=1., high=10., size=(s, s)) for i in range(k)]
    return mu,sig



if __name__ == "__main__":
    #data = randomWeights(10,3)
    #print sum(data[:,2])

    data1 = pd.read_csv("dataset2.csv",
                        delim_whitespace=True).values
    #print data1[0][1]
    x = []
    y = []
    for d in data1:
        x.append(d[0])
        y.append(d[1])
    # plt.scatter(x,y)
    # plt.show()
    data = np.column_stack([x, y])
    #print data
    #print data[:, 0]
    data = np.array(data)
    #print data[:, 0]

    lls = gaussian_mixture(data,3,1,0.001,2000,0,1)
    plotLikelihood(lls)